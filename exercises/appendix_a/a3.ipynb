{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ab56de",
   "metadata": {},
   "source": [
    "# A.3 Seeing models as computation graphs\n",
    "\n",
    "Computational graphs are directed graphs representing the mathematical operations of a model. In Deep Learning, these graphs layout the sequence of calculations needed to compute the output of a neural network. The graph is used by PyTorch's autograd system to automatically compute gradients of model parameters, meaning that it traces back the partial derivatives of each operation starting from the loss function resulting in gradients that can be used to update parameters, weights, and biases to improve the model's performance.\n",
    "\n",
    "## A Logistic Regression Forward Pass\n",
    "\n",
    "Logistic Regression: A continuous number 'z' is predicted (regression) and then squashed to a probability between 0 and 1 via the logistic (sigmoid) function in order to compare it to the true label (target) and calculate the loss.\n",
    "\n",
    "## 1. Define the data and parameters\n",
    "\n",
    "We define sample tensors for the label, input, weights and biases with the `required_grad` flag set to true to enable PyTorch's autograd feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0,]) # true label (target $y$ ground truth)\n",
    "# In a real dataset, y would be a long vector of many labels (e.g., [1.0, 0.0, 1.0, 1.0...]). \n",
    "# Just keep in mind that the \"shape\" of your label tensor must always match the \"shape\" of your prediction tensor.\n",
    "x1 = torch.tensor([1.1]) # input feature\n",
    "w1 = torch.tensor([2.2], requires_grad=True) # weight (learned parameter)\n",
    "b = torch.tensor([0.0], requires_grad=True) # bias (learned parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274b03c",
   "metadata": {},
   "source": [
    "## 2. Calculate the \"Net Input\" (Linear Combination)\n",
    "\n",
    "In PyTorch, the value z (the raw output before sigmoid) is often called the \"Logits.\" Because `$z$` is the value that would result if you ran the Logit function on the final probability, we call `$z$` the \"Logit.\" In Deep Learning, the convention has become: \"Logits = the raw, un-normalized scores produced by the last layer of a model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = x1 * w1 + b # input times weight plus bias (net input)\n",
    "# z = (1.1 * 2.2) + 0.0 = 2.42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb35182",
   "metadata": {},
   "source": [
    "Linear combination can be any number from negative infinity to positive infinity. The sigmoid function is a non-linear function that maps the linear combination to a probability between 0 and 1.\n",
    "\n",
    "## 3. Apply the Logistic Function (Sigmoid)\n",
    "\n",
    "This converts 2.42 into a probability between 0 and 1 (approx 0.918). In statistics, \"Logit\" is the inverse of the sigmoid function as defined above (the `$z$` linear combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.sigmoid(z) # activation function prediction ($\\hat{y}$ aka \"y-hat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1f464",
   "metadata": {},
   "source": [
    "## 4. Calculate the Loss\n",
    "\n",
    "Compares the prediction (0.918) to the true label (1.0). Simple substraction is not used because Binary Cross Entropy (BCE) uses logarithms to heavily punish confident wrong answers. If the label is 1.0 and the model predicts 0.9, the loss is small. If the label is 1.0 and the model predicts 0.001, the log function makes the loss massive to signal a need to update weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.binary_cross_entropy(a, y) # loss function"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
